#인공지능 기초를 위한 FAQ
**1. 인공지능에서 지능에 해당하는 기능은 무엇인가?**
데이터에 따라 파라미터를 바꿔가며 학습하는 것

**2. 인공지능의 종류 3가지에 대해서 설명하시오 (지도학습, 반지도학습, 강화학습)
지도학습 (Supervised Learning)** 
학습 시 사용하는 데이터 샘플과 대응되는 정답 값이 존재하고, 정답 값과 예측 값의 차이를 인풋-아웃풋 상관관계 모델링
비지도 학습 (Unsupervised Learning)
학습 시 정답 값을 사용하지 않는 기조, 데이터의 차원을 축소하여 클러스터링 하거나, 보조 태스크를 설계하여 정답 값 없이 유의미한 feature을 뽑아낼 수 있도록 하는 self-supervise learning이 포함됨
반지도 학습 (Semi-Supervised Learning)
정답 값이 있는 데이터 샘플과 정답 값이 없는 데이터 샘플을 둘 다 사용하는 기법을 총망라하는 용어, 보통 정답 값이 있는 데이터 샘플을 가지고 훈련된 모델을 이용해 정답 값이 없는 레이블에 추정 정답 값을 넣는 self-labeling기법이 사용됨

**3. 전통적인 프로그래밍 방법과 인공지능 프로그램의 차이점은 무엇인가?**
전통적인 프로그래밍 방법은 특정 task를 풀기 위하여 프로그래머가 직접 룰을 세우고 구현 – rule based algorithm, 인공지능 프로그램은 특정 task를 풀기 위하여 프로그램이 데이터 간 상관관계를 모델링 한다

**4. 딥러닝과 머신러닝의 차이점은 무엇인가?**
딥러닝은 머신러닝 범주 안에 있음, 고전적인 머신러닝 기법과 딥러닝 기법의 차이는 실험자가 feature engineering, feature selection 같은 과정을 가지는 지 유무에 있다. 딥러닝은 레이어를 다층으로 쌓아 feature extraction을 하는데, implicit 하게 데이터 간 상관관계를 잘 찾을 것 이라는 가정하에 있다.

**5. Classification과 Regression의 주된 차이점은?**
Classification은 discrete value를 출력해야 하는 task이고, Regression은 continuous value를 출력해야 하는 task이다.

**6. 머신러닝에서 차원의 저주(curse of dimensionality)란?**
Feature의 차원이 너무 많아 모델이 의미없는 feature에 방해받아 성능이 저하되는 현상
데이터 포인트가 고차원 공간에서 고르게 분포하기 어려워져 모든 포인트가 서로 멀리 떨어져 있게 됨, 훈련 데이터의 밀도가 낮아져, 학습이 어려워지고 모델이 일반화 하기 어려워짐, 차원이 증가하면서 유클리드 거리 같은 거리 계산이 왜곡될 수 있게 됨 -> 가장 가까운 이웃 찾기가 어려워짐, 계산 복잡도 증가 – 특히 k-NN같은 알고리즘은 성능이 크게 저하
Dimension을 줄이기 위해 PCA, Feature Selection

**7. Dimensionality Reduction는 왜 필요한가?**
고차원의 feature을 사용하면 노이즈 양로 비례하여 늘어나므로 차원을 줄여 noise reduction의 효과도 볼 수 있음, 계산 비용 줄어듬

**8. Ridge Lasso의 공통점과 차이점? (Regularization, 규제, Scaling)**
L1 정규화 Lasso Regularization
모델의 손실함수(Loss Function)에 가중치 절대값의 합을 추가하는 방식
L1-Regularized Loss = Loss + λ∑∣wi∣
특징 : 불필요한 가중치들을 0으로 만들어 희소성을 유도, 특징이 많을 때, 중요하지 않은 특징들을 제거(Feature Selection 효과).
장점 : 가중치가 완전히 0이 될 수 있어 모델의 해석력이 증가, 희소 데이터나 Feature Selection이 중요한 경우 효과적
X, y가 독립적일 때

L2 Ridge Regularization
L2 정규화는 손실 함수에 가중치 제곱 값의 합을 추가하는 방식
L2-Regularized Loss=Loss+λ∑wi2
특징 : 큰 가중치에 페널티를 부여하여 Overfitting 방지, 가중치를 0에 가깝게 만드나 완전히 0으로 만들지는 않음
장점 : 모델의 안정성과 일반화 성능 향상, 모든 특징을 고려하되 가중치를 축소하여 복잡도 줄임
X, Y가 유의미한 관계가 있을 때

공통점 : 모델이 overfitting하지 않도록 가중치를 줄이는 역할

**9. Overfitting vs. Underfitting**
Overfitting
발생 이유 : 모델이 너무 복잡(파라미터 수가 많거나 깊은 신경망을 사용할 경우), 데이터 부족, 불필요하거나 무작위적인 노이즈가 많을 때 모델이 패턴으로 오인하고 학습함, 불균형 데이터, 과도한 반복(에포크가 너무 많음), 유의미한 특징 부족 또는 불필요한 특징 포함
해결 방법 : 데이터 증량, 정규화(L1, L2정규화 또는 드롭 아웃), 모델의 복잡도 줄이기(파라미터 수 줄이거나 간단한 모델 사용), Early Stopping : 훈련 데이터를 반복 학습하다가 검증 데이터의 성능이 더 이상 개선되지 않을 때 학습 중단, 교차 검증, Feature Selection
Underfitting
발생 이유 : 단순한 모델, 학습 부족, 데이터 특징 부족, 잘못된 모델(알고리즘) 선택, 과도한 정규화, 데이터 부족
해결 방법 : 모델 복잡도 증가, 학습 시간 확보, essential feature 추가, 데이터 증가, 데이터 전처리 개선, 정규화 강도 감소, 하이퍼파라미터 튜닝

**10. Feature Engineering과 Feature Selection의 차이점은?**
Feature engineering
데이터의 feature을 선택, 변환, 생성, 조합하는 과정
모델 성능 향상, 복잡도 감소, 데이터 표현 개선, 학습 속도 향상
Feature Selection, Feature Transformation(변환 – 스케일링, 원핫인코딩), Feature Extraction(추출 – 원본 데이터를 사용해 유의미한 특성을 새로 생성), Feature Reduction(축소 – 차원이 높은 데이터를 압축하여 간결한 표현 생성, PCA, t-SNE 등)
Feature selection
특정 task를 수행하는 것에 essential한 feature을 선택하는 것, 있는 feature 중에서만 선택한다
Feature engineering은 새로운 특성을 만들어내는 것, Feature Selection은 불필요한 ㅌ특성을 제거하는 것

**11. 전처리(Preprocessing)의 목적과 방법? (노이즈, 이상치, 결측치)**
전처리는 모델이 더 잘 학습할 수 있도록 데이터를 정제하고 변환하는 일
모델 향상, 데이터 품질 향상, 데이터 일관성 유지, feature 강화
노이즈 처리 : 필터링, Smoothening, 특성 스케일링
이상치 처리 : 이상치 탐지, 이상치 제거 교체, 로버스트 모델 사용
결측치 처리 : 결측치 삭제, 대체, 표시
스케일링, 범주형 데이터 처리(원핫 인코딩)

**12. EDA(Explorary Data Analysis)란? 데이터의 특성 파악(분포, 상관관계)**
데이터를 이해하고 모델링 전략을 세우고 적합한 알고리즘 선택
데이터 구조 이해, 상관관계 분석, 데이터 간 관계 시각화 – 분포 파악, 데이터 변환 및 특성 생성

**13. 회귀에서 절편과 기울기가 의미하는 바는? 딥러닝과 어떻게 연관되는가?**
단순한 선형 회귀를 예로 들어, y = ax + b
기울기 a : 인풋 – 아웃풋 간 상관관계의 방향 및 강도에 대한 값
	A > 0이면 양의 상관관계 ~~ 기울기의 크기는 상관관계 강도
Y 절편 b : x가 0일 때의 값이므로 시작점이 됨, x가 0일 가능성이 높을 때 유의미할 수 있음
딥러닝은 퍼셉트론을 여러 층으로 쌓은 것이고, 퍼셉트론은 y = ax + b로 표현할 수 있음

**28. 결정트리에서 불순도(Impurity) - 지니 계수(Gini Index)란 무엇인가?**
특정 데이터 군집 안에서의 데이터 포인트가 같은 클래스로 구성되어 있는 정도, impurity를 1로 올리는 것이 좋음
남자 50, 여자 50 이 있다면 impurity는 0.5

**29. 앙상블이란 무엇인가?**
다른 하이퍼파라미터를 가진 여러 개의 모델을 동시에 훈련하여 추론에 사용하는 기법
대표적인 예시로 random forest 가 있다

**30. 부트 스트랩핑(bootstraping)이란 무엇인가?**
Bootstrapping은 데이터에서 복원 추출을 통해 여러 개의 샘플을 생성하여 통계적 추정이나 모델 학습에 활용하는 기법이다.
복원추출: 원본 데이터에서 임의로 샘플을 뽑되, 한 번 뽑힌 데이터도 다시 뽑힐 수 있음
반복적으로 샘플링하여 새로운 데이터셋 생성
작은 데이터셋에도 안정적인 결과를 얻을 수 있도록 함

**31. 배깅(Bagging)이란 무엇인가?**
Bagging (Bootstraping & aggregation) : 데이터 샘플링에 중복을 허용하여 여러 개의 데이터 스플릿을 만드는 것 <->boosting

**32. 주성분 분석(PCA) 이란 무엇인가?**
고차원 데이터를 저차원으로 변환하는 차원 축소 기법 중 하나
데이터의 분산을 최대화 하여, 데이터의 본질적인 특성을 잘 나타낼 수 있는 주성분을 찾음
목적, 장점 : 고차원의 데이터를 저차원의 데이터로 나타내면서 데이터의 중요한 정보는 보존하는 것 -> 데이터 시각과(고차원의 데이터를 2D 또는 3D로 시각화), 계산 효율성, 잡음 제거
단점 : 선형 차원 축소 기법이기 때문에 데이터의 관계가 비선형일 경우 적합하지 않음, 축소된 데이터의 각 축이 원본 특성들과 어떻게 연관되는지 해석하는 것이 어려울 수 있다

